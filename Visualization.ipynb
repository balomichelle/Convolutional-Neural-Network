{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data type of X is  <class 'numpy.ndarray'>\n",
      "The data type of y is  <class 'numpy.ndarray'>\n",
      "The shape of X is  (1000, 28, 28, 1)\n",
      "The shape of y is  (1000,)\n",
      "The number of unique classes of y is  10\n",
      "The shape of X_train is  (800, 28, 28, 1)\n",
      "The shape of y_train is  (800,)\n",
      "Rows of y [5 0 4 1 9]\n",
      "Rows of y_oh after np.zeros transformation  [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Shape of y_oh after np.zeros transformation  (1000, 10)\n",
      "Shape of y_oh_train after np.zeros transformation  (800, 10)\n",
      "Shape of y_oh_test after np.zeros transformation  (200, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "# for the random seed\n",
    "import tensorflow as tf\n",
    "\n",
    "# set the random seeds to get reproducible results\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(2)\n",
    "\n",
    "# Load data from https://www.openml.org/d/554\n",
    "#X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "tmp = np.load('mnist.npz')\n",
    "X = tmp['X']\n",
    "y = tmp['y']\n",
    "X, y = X[:1000], y[:1000]\n",
    "X = X.reshape(X.shape[0], 28, 28, 1)\n",
    "\n",
    "# To look at the data type\n",
    "\n",
    "print('The data type of X is ', type(X))\n",
    "print('The data type of y is ', type(y))\n",
    "\n",
    "# To look at the shape of X and Y\n",
    "print('The shape of X is ', X.shape)\n",
    "print('The shape of y is ', y.shape)\n",
    "\n",
    "# Normalize\n",
    "X = X / 255.\n",
    "# number of unique classes\n",
    "num_classes = len(np.unique(y))\n",
    "print ('The number of unique classes of y is ', num_classes )\n",
    "y = y.astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n",
    "\n",
    "print('The shape of X_train is ', X_train.shape)\n",
    "print('The shape of y_train is ', y_train.shape)\n",
    "\n",
    "num_tot = y.shape[0]\n",
    "num_train = y_train.shape[0]\n",
    "num_test = y_test.shape[0]\n",
    "\n",
    "y_oh = np.zeros((num_tot, num_classes))\n",
    "y_oh[range(num_tot), y] = 1\n",
    "\n",
    "# Print out a few rows of y.\n",
    "print('Rows of y', y[0:5] )\n",
    "# Print out a few rows of y_oh.\n",
    "print('Rows of y_oh after np.zeros transformation ', y_oh[0:5] )\n",
    "# Print out the shape of y_oh.\n",
    "print('Shape of y_oh after np.zeros transformation ', y_oh.shape )\n",
    "\n",
    "y_oh_train = np.zeros((num_train, num_classes))\n",
    "y_oh_train[range(num_train), y_train] = 1\n",
    "\n",
    "y_oh_test = np.zeros((num_test, num_classes))\n",
    "y_oh_test[range(num_test), y_test] = 1\n",
    "\n",
    "# Print out the shape of y_oh_train and y_oh_test.\n",
    "print('Shape of y_oh_train after np.zeros transformation ', y_oh_train.shape )\n",
    "print('Shape of y_oh_test after np.zeros transformation ', y_oh_test.shape )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               102528    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 108,618\n",
      "Trainable params: 108,618\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.2120 - val_loss: 1.9519\n",
      "Epoch 2/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.1937 - val_loss: 0.7618\n",
      "Epoch 3/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.4785 - val_loss: 0.6588\n",
      "Epoch 4/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.3208 - val_loss: 0.4658\n",
      "Epoch 5/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.2129 - val_loss: 0.4705\n",
      "Epoch 6/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.1693 - val_loss: 0.4526\n",
      "Epoch 7/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.1190 - val_loss: 0.5314\n",
      "Epoch 8/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0876 - val_loss: 0.4368\n",
      "Epoch 9/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0687 - val_loss: 0.3844\n",
      "Epoch 10/150\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0444 - val_loss: 0.4139\n",
      "Epoch 11/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0368 - val_loss: 0.4745\n",
      "Epoch 12/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0317 - val_loss: 0.4341\n",
      "Epoch 13/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0208 - val_loss: 0.5211\n",
      "Epoch 14/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0187 - val_loss: 0.4581\n",
      "Epoch 15/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0150 - val_loss: 0.4677\n",
      "Epoch 16/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0102 - val_loss: 0.4561\n",
      "Epoch 17/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0119 - val_loss: 0.4461\n",
      "Epoch 18/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0100 - val_loss: 0.5013\n",
      "Epoch 19/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0045 - val_loss: 0.4852\n",
      "Epoch 20/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0033 - val_loss: 0.5110\n",
      "Epoch 21/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0029 - val_loss: 0.5045\n",
      "Epoch 22/150\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.0024 - val_loss: 0.5065\n",
      "Epoch 23/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0022 - val_loss: 0.5068\n",
      "Epoch 24/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0020 - val_loss: 0.5101\n",
      "Epoch 25/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0019 - val_loss: 0.5124\n",
      "Epoch 26/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0017 - val_loss: 0.5190\n",
      "Epoch 27/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0016 - val_loss: 0.5252\n",
      "Epoch 28/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0016 - val_loss: 0.5200\n",
      "Epoch 29/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0015 - val_loss: 0.5304\n",
      "Epoch 30/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0014 - val_loss: 0.5235\n",
      "Epoch 31/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0013 - val_loss: 0.5279\n",
      "Epoch 32/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0012 - val_loss: 0.5265\n",
      "Epoch 33/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0012 - val_loss: 0.5280\n",
      "Epoch 34/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0011 - val_loss: 0.5361\n",
      "Epoch 35/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0011 - val_loss: 0.5347\n",
      "Epoch 36/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0010 - val_loss: 0.5353\n",
      "Epoch 37/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0010 - val_loss: 0.5384\n",
      "Epoch 38/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 9.3827e-04 - val_loss: 0.5378\n",
      "Epoch 39/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 8.8255e-04 - val_loss: 0.5428\n",
      "Epoch 40/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 8.3831e-04 - val_loss: 0.5420\n",
      "Epoch 41/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 8.2815e-04 - val_loss: 0.5406\n",
      "Epoch 42/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 7.8832e-04 - val_loss: 0.5483\n",
      "Epoch 43/150\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 7.6522e-04 - val_loss: 0.5411\n",
      "Epoch 44/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 7.3800e-04 - val_loss: 0.5473\n",
      "Epoch 45/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 7.3295e-04 - val_loss: 0.5468\n",
      "Epoch 46/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 7.0028e-04 - val_loss: 0.5495\n",
      "Epoch 47/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 6.7637e-04 - val_loss: 0.5487\n",
      "Epoch 48/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 6.5362e-04 - val_loss: 0.5531\n",
      "Epoch 49/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 6.3842e-04 - val_loss: 0.5512\n",
      "Epoch 50/150\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 6.2477e-04 - val_loss: 0.5508\n",
      "Epoch 51/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 6.0404e-04 - val_loss: 0.5570\n",
      "Epoch 52/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 5.8106e-04 - val_loss: 0.5530\n",
      "Epoch 53/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 5.7355e-04 - val_loss: 0.5578\n",
      "Epoch 54/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 5.5708e-04 - val_loss: 0.5544\n",
      "Epoch 55/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 5.3781e-04 - val_loss: 0.5549\n",
      "Epoch 56/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 5.2992e-04 - val_loss: 0.5587\n",
      "Epoch 57/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 5.1891e-04 - val_loss: 0.5593\n",
      "Epoch 58/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 5.1060e-04 - val_loss: 0.5586\n",
      "Epoch 59/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 4.9920e-04 - val_loss: 0.5625\n",
      "Epoch 60/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 4.8505e-04 - val_loss: 0.5628\n",
      "Epoch 61/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 4.6844e-04 - val_loss: 0.5621\n",
      "Epoch 62/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 4.6315e-04 - val_loss: 0.5631\n",
      "Epoch 63/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 4.5138e-04 - val_loss: 0.5632\n",
      "Epoch 64/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 4.4574e-04 - val_loss: 0.5662\n",
      "Epoch 65/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 4.3311e-04 - val_loss: 0.5679\n",
      "Epoch 66/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 4.2629e-04 - val_loss: 0.5665\n",
      "Epoch 67/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 4.1450e-04 - val_loss: 0.5686\n",
      "Epoch 68/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 4.0890e-04 - val_loss: 0.5695\n",
      "Epoch 69/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 3.9643e-04 - val_loss: 0.5681\n",
      "Epoch 70/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 3.9412e-04 - val_loss: 0.5724\n",
      "Epoch 71/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 3.8182e-04 - val_loss: 0.5689\n",
      "Epoch 72/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 3.8046e-04 - val_loss: 0.5700\n",
      "Epoch 73/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 3.7177e-04 - val_loss: 0.5724\n",
      "Epoch 74/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 3.6258e-04 - val_loss: 0.5731\n",
      "Epoch 75/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 3.5536e-04 - val_loss: 0.5705\n",
      "Epoch 76/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 3.5330e-04 - val_loss: 0.5711\n",
      "Epoch 77/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 3.4282e-04 - val_loss: 0.5741\n",
      "Epoch 78/150\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 3.3914e-04 - val_loss: 0.5752\n",
      "Epoch 79/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 3.3218e-04 - val_loss: 0.5776\n",
      "Epoch 80/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 3.2624e-04 - val_loss: 0.5754\n",
      "Epoch 81/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 3.2280e-04 - val_loss: 0.5757\n",
      "Epoch 82/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 3.1598e-04 - val_loss: 0.5760\n",
      "Epoch 83/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 3.1606e-04 - val_loss: 0.5794\n",
      "Epoch 84/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 3.0773e-04 - val_loss: 0.5771\n",
      "Epoch 85/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 3.0194e-04 - val_loss: 0.5780\n",
      "Epoch 86/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 3.0147e-04 - val_loss: 0.5785\n",
      "Epoch 87/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.9324e-04 - val_loss: 0.5805\n",
      "Epoch 88/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.8951e-04 - val_loss: 0.5809\n",
      "Epoch 89/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.8554e-04 - val_loss: 0.5809\n",
      "Epoch 90/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.8208e-04 - val_loss: 0.5813\n",
      "Epoch 91/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.7864e-04 - val_loss: 0.5814\n",
      "Epoch 92/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.7415e-04 - val_loss: 0.5835\n",
      "Epoch 93/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.7052e-04 - val_loss: 0.5815\n",
      "Epoch 94/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.6886e-04 - val_loss: 0.5832\n",
      "Epoch 95/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.6397e-04 - val_loss: 0.5831\n",
      "Epoch 96/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.5763e-04 - val_loss: 0.5853\n",
      "Epoch 97/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.5378e-04 - val_loss: 0.5854\n",
      "Epoch 98/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.5176e-04 - val_loss: 0.5842\n",
      "Epoch 99/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.4736e-04 - val_loss: 0.5870\n",
      "Epoch 100/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.4634e-04 - val_loss: 0.5853\n",
      "Epoch 101/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.4264e-04 - val_loss: 0.5853\n",
      "Epoch 102/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.3865e-04 - val_loss: 0.5872\n",
      "Epoch 103/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.3619e-04 - val_loss: 0.5874\n",
      "Epoch 104/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.3233e-04 - val_loss: 0.5876\n",
      "Epoch 105/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.3008e-04 - val_loss: 0.5884\n",
      "Epoch 106/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.2782e-04 - val_loss: 0.5883\n",
      "Epoch 107/150\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 2.2358e-04 - val_loss: 0.5894\n",
      "Epoch 108/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.2286e-04 - val_loss: 0.5909\n",
      "Epoch 109/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.1928e-04 - val_loss: 0.5899\n",
      "Epoch 110/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.1828e-04 - val_loss: 0.5893\n",
      "Epoch 111/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.1397e-04 - val_loss: 0.5901\n",
      "Epoch 112/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.1202e-04 - val_loss: 0.5926\n",
      "Epoch 113/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.0864e-04 - val_loss: 0.5899\n",
      "Epoch 114/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.0690e-04 - val_loss: 0.5929\n",
      "Epoch 115/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.0559e-04 - val_loss: 0.5938\n",
      "Epoch 116/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.0187e-04 - val_loss: 0.5925\n",
      "Epoch 117/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 2.0078e-04 - val_loss: 0.5934\n",
      "Epoch 118/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.9718e-04 - val_loss: 0.5935\n",
      "Epoch 119/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.9550e-04 - val_loss: 0.5939\n",
      "Epoch 120/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.9435e-04 - val_loss: 0.5959\n",
      "Epoch 121/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.9154e-04 - val_loss: 0.5932\n",
      "Epoch 122/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.8937e-04 - val_loss: 0.5963\n",
      "Epoch 123/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.8686e-04 - val_loss: 0.5954\n",
      "Epoch 124/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.8580e-04 - val_loss: 0.5956\n",
      "Epoch 125/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.8381e-04 - val_loss: 0.5948\n",
      "Epoch 126/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.8147e-04 - val_loss: 0.5967\n",
      "Epoch 127/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.7953e-04 - val_loss: 0.5971\n",
      "Epoch 128/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.7859e-04 - val_loss: 0.5968\n",
      "Epoch 129/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.7716e-04 - val_loss: 0.5986\n",
      "Epoch 130/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.7597e-04 - val_loss: 0.5980\n",
      "Epoch 131/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.7318e-04 - val_loss: 0.5979\n",
      "Epoch 132/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.7188e-04 - val_loss: 0.5996\n",
      "Epoch 133/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.6991e-04 - val_loss: 0.5974\n",
      "Epoch 134/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.6902e-04 - val_loss: 0.5983\n",
      "Epoch 135/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.6677e-04 - val_loss: 0.5987\n",
      "Epoch 136/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.6511e-04 - val_loss: 0.6006\n",
      "Epoch 137/150\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.6321e-04 - val_loss: 0.6008\n",
      "Epoch 138/150\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.6327e-04 - val_loss: 0.5998\n",
      "Epoch 139/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.6181e-04 - val_loss: 0.6014\n",
      "Epoch 140/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.5916e-04 - val_loss: 0.6004\n",
      "Epoch 141/150\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.5765e-04 - val_loss: 0.6010\n",
      "Epoch 142/150\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.5705e-04 - val_loss: 0.6010\n",
      "Epoch 143/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.5441e-04 - val_loss: 0.6019\n",
      "Epoch 144/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.5359e-04 - val_loss: 0.6038\n",
      "Epoch 145/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.5228e-04 - val_loss: 0.6034\n",
      "Epoch 146/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.5086e-04 - val_loss: 0.6027\n",
      "Epoch 147/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.4954e-04 - val_loss: 0.6042\n",
      "Epoch 148/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.4790e-04 - val_loss: 0.6031\n",
      "Epoch 149/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.4720e-04 - val_loss: 0.6045\n",
      "Epoch 150/150\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 1.4611e-04 - val_loss: 0.6038\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff15c6804e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "# Max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "# Max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "\n",
    "# Resetthe model first\n",
    "# model.reset_states()\n",
    "\n",
    "history = model.fit(X, y_oh, validation_split = 0.2, batch_size=32, epochs=150, verbose = 1)\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
