{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI 1 - Assignment 2\n",
    "\n",
    "This assignment requires you to install [Keras](https://keras.io/) and [Tensorflow](https://www.tensorflow.org). Keras is a high-level Deep Learning API, written in Python using TensorFlow, CNTK, or Theano as back-ends. Here, we will use Tensorflow as back-end.\n",
    "\n",
    "This assignment is divided in two parts. In the first part you will learn about Keras with the help of the example below and the Keras [documentation](https://keras.io/). In the second part, you will practise training a Deep Learning model.\n",
    "\n",
    "## How to submit\n",
    "Submit this notebook with **plots**, **results** and **code** showing how the results were genereated. This assignment can either be submitted by dropping it in your student folder (on the Schulich NAS) or upload to it the CMD. Remember to name your files appropriately.\n",
    "**Deadline** is on 17:30 on November 15, 2018.\n",
    "\n",
    "## Installation\n",
    "Instructions can be found here:\n",
    "* [Keras](https://keras.io)\n",
    "* [Tensorflow](https://www.tensorflow.org/install/)\n",
    "\n",
    "I recommend using ```pip```. For Tensorflow is it sufficient to install the CPU version. The GPU version requires a good workstation with high-end Nvidia GPU(s), and it is not necessary for this assignment.\n",
    "\n",
    "If you're using a virtualenv:\n",
    "```\n",
    "pip3 install keras\n",
    "```\n",
    "and \n",
    "```\n",
    "pip3 install tensorflow\n",
    "```\n",
    "Add ```sudo``` for a systemwide installation (i.e. no ```virtualenv```).\n",
    "```\n",
    "sudo pip3 install keras\n",
    "```\n",
    "and \n",
    "```\n",
    "sudo pip3 install tensorflow\n",
    "```\n",
    "\n",
    "## Part 1 - understand a model\n",
    "\n",
    "### Optimizers\n",
    "\n",
    "Loss is the penalty for a bad prediction. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater than zero. The goal of training a model is to find a set of weights and biases (i.e. parameters) that have, on average, a low loss across all examples. The term cost is used interchangably with loss. See the [loss section](https://keras.io/losses/) in the Keras documentation for a list and descriptions of what is available.\n",
    "\n",
    "<img src=\"./LossSideBySide.png\" width=\"500\">\n",
    "<figcaption>Figure. Left: high loss and right: low loss.</figcaption>\n",
    "\n",
    "The optimizer is the algorithm used to minimize the loss/cost. Optimizers in neural networks work by finding the gradient/derivative of the loss with respect to the parameters (i.e. the weights). \"Gradient\" is the correct term since a we are looking at multi-dimensional systems (i.e. many parameters), however, the terms are often used interchangably. For those who didn't take multivariable calculus, just think of the gradient as a derivative. The derivative of the loss with respect to a parameters tells us how much the loss changes when we nudge a weight up or down. So, by knowing how a given parameter affects the loss the optimizer can change it so as to decrease the loss. The various optimizers differ in how they change the weights. \n",
    "\n",
    "#### Mini-overview over popular optimizers\n",
    "\n",
    "* **Stochastic Gradient Descent (SGD)**. This is the most basic and easy to understand optimizer. It updates the weights in the negative direction of the gradient by taking the average gradient of mini-batch of data (e.g. 20-50 exemplars) in each step. Vanilla SGD only has one hyper-parameter, the learning rate.\n",
    "* **Momentum**. This optimizer \"gains speed\" when the gradient has pointed in the same direction for several consecutive updates. That is, it gains momentum. It does this by accumulating an exponentially decaying moving average of past gradients. The step size depends on how large and aligned the sequence of gradients are. The most important hyper-parameter is alpha and common values are 0.5 and 0.9.\n",
    "* **Nesterov Momentum**. This is a modification of the standard momentum optimizer.\n",
    "* **AdaGrad**. This optimizer Ada-ptively sets the learning rate depending on the steepness/magnitude of the Grad-ients. This is done so that weights with big gradients get a smaller effective learning rate, and weights with small gradients will get a greater effective learning rate. The result is quicker progress in the more gently sloped directions of the weight space and a slowdown in stepp regions.\n",
    "* **RMSProp**. This is modification of AdaGrad, where the accumulated gradient decays, that is, the influence of previous gradients gradually decreases.\n",
    "* **Adam**. The name comes from \"adaptive moments\", and it is a combination of RMSProp and momentum. It has several hyper-parameters.\n",
    "\n",
    "The above list just gives a quick overview of some of the most common. However, old optimizers are constantly improved and new are developed. SGD and momentum are most basic and easiest to understand and implement. They are still in use, but the more advanced optimizers tend to be better for practical use. Which one to use is generally an emperical question depending on both the data and the model.\n",
    "\n",
    "For a more complete overview of optimization algorithms see [this comparison](http://ruder.io/optimizing-gradient-descent/), and to see what is available in Keras, see the [optimizer section](https://keras.io/optimizers/) of the documentation.\n",
    "\n",
    "See the images below for a comparison of optimizers in a 2D space (NAG: Nesterov accelerated gradient, Adadelta: an extension of AdaGrad).\n",
    "\n",
    "<img src=\"./contours_evaluation_optimizers.gif\" width=\"500\">\n",
    "<img src=\"./saddle_point_evaluation_optimizers.gif\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows of y [5 0 4 1 9]\n",
      "Rows of y_oh after np.zeros transformation  [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "# for the random seed\n",
    "import tensorflow as tf\n",
    "\n",
    "# set the random seeds to get reproducible results\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(2)\n",
    "\n",
    "import numpy as np\n",
    "tmp = np.load('mnist.npz')\n",
    "X = tmp['X']\n",
    "y = tmp['y']\n",
    "X, y = X[:1000], y[:1000]\n",
    "\n",
    "# To plot three examples from the dataset\n",
    "plt.subplot(221)\n",
    "plt.imshow(X[0], cmap=plt.get_cmap('gray'))\n",
    "plt.title('Examples from the Minst Dataset')\n",
    "plt.subplot(222)\n",
    "plt.imshow(X[1], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(X[2], cmap=plt.get_cmap('gray'))\n",
    "plt.show()\n",
    "\n",
    "# Reshape the X arrays\n",
    "X = X.reshape(X.shape[0], 28, 28, 1)\n",
    "\n",
    "# Normalize\n",
    "X = X / 255.\n",
    "# number of unique classes\n",
    "num_classes = len(np.unique(y))\n",
    "y = y.astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n",
    "\n",
    "num_tot = y.shape[0]\n",
    "num_train = y_train.shape[0]\n",
    "num_test = y_test.shape[0]\n",
    "\n",
    "y_oh = np.zeros((num_tot, num_classes))\n",
    "y_oh[range(num_tot), y] = 1\n",
    "\n",
    "# Print out a few rows of y.\n",
    "print('Rows of y', y[0:5] )\n",
    "# Print out a few rows of y_oh.\n",
    "print('Rows of y_oh after np.zeros transformation ', y_oh[0:5] )\n",
    "\n",
    "y_oh_train = np.zeros((num_train, num_classes))\n",
    "y_oh_train[range(num_train), y_train] = 1\n",
    "\n",
    "y_oh_test = np.zeros((num_test, num_classes))\n",
    "y_oh_test[range(num_test), y_test] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "**The data set**\n",
    "\n",
    "Plot a three examples from the data set.\n",
    "* What type of data are in the data set?\n",
    "* What does the line ```X = X.reshape(X.shape[0], 28, 28, 1)``` do?\n",
    "\n",
    "Look at how the encoding of the targets (i.e. ```y```) is changed. E.g. the lines\n",
    "```\n",
    "y_oh = np.zeros((num_tot, num_classes))\n",
    "y_oh[range(num_tot), y] = 1\n",
    "```\n",
    "Print out a few rows of ```y``` next to ```y_oh```.\n",
    "* What is the relationship between ```y``` and ```y_oh```?\n",
    "* What is the type of encoding in ```y_oh``` called and why is it used?\n",
    "* Plot three data examples in the same figure and set the correct label as title. \n",
    "    * It should be possible to see what the data represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What type of data are in the data set?\n",
    "# The mnist dataset contains numpy array data. Their are handwritten digit images represented by digits\n",
    "# The training dataset X is structured as a 3-dimensional array of instance, image width and image height. \n",
    "\n",
    "# What does the line X = X.reshape(X.shape[0], 28, 28, 1) do?\n",
    "# This line reshapes the original array to a 3D array, which has (28,28,1) in 3D array shape.\n",
    "\n",
    "# What is the relationship between y and y_oh?\n",
    "# y_oh is another representation of y.\n",
    "# The index of value 1 in numpy array y_oh \n",
    "# is the same as the value of y\n",
    "\n",
    "# What is the type of encoding in y_oh called and why is it used?\n",
    "# One Hot Encoding. The response variables have ten different classes. Changing the 1 to 10 values into one hot encoding\n",
    "# can make the loss funcion meaningful, because there is no point to compare the predicted value \n",
    "# with different numbers from 1 - 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "**The model**\n",
    "\n",
    "Below is some code for bulding and training a model with Keras.\n",
    "* What type of network is implemented below? I.e. a normal MLP, RNN, CNN, Logistic Regression...?\n",
    "* What does ```Dropout()``` do?\n",
    "* Which type of activation function is used for the hidden layers?\n",
    "* Which type of activation function is used for the output layer?\n",
    "* Why are two different activation functions used?\n",
    "* What optimizer is used in the model below?\n",
    "* How often are the weights updated (i.e. after how many data examples)?\n",
    "* What loss function is used?\n",
    "* How many parameters (i.e. weights and biases, NOT hyper-parameters) does the model have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What type of network is implemented below? I.e. a normal MLP, RNN, CNN, Logistic Regression...?\n",
    "# CNN, Convolutional Neural Network\n",
    "\n",
    "#What does Dropout() do?\n",
    "# It's a regularization layer. \n",
    "# It is configured to randomly exclude a portion of neurons in the layer in order to reduce overfitting.\n",
    "\n",
    "# Which type of activation function is used for the hidden layers?\n",
    "# A rectifier activation function (Relu)\n",
    "\n",
    "# Which type of activation function is used for the output layer?\n",
    "# A softmax activation function\n",
    "\n",
    "# Why are two different activation functions used?\n",
    "# They have different functions in the convolutional neural network.\n",
    "# In the input layer, a rectifier activation funciton serves to forward propagate the signal from the input layer through the network.\n",
    "# It's main purpose it to convert a input signal of a node to an output signal, which will be used as an input signal in the next layer.\n",
    "# In the output layer, a softmax activation function serves to output probability-like predictions for each class.\n",
    "# Softmax function is good for classification, while Relu function is not.\n",
    "\n",
    "# What optimizer is used in the model below?\n",
    "# Stochatic Gradient Descent is used to learn the weights.\n",
    "\n",
    "# How often are the weights updated (i.e. after how many data examples)?\n",
    "# The model is fit over 60 epochs with updates every 32 data examples.\n",
    "# In this network, it takes the first 32 examples from the training dataset and trains the network.\n",
    "# It keeps doing this procedure until we have propagated through all samples of the network.\n",
    "# In this network, we have 25 batches. We update the weights after each propagation.\n",
    "\n",
    "# What loss function is used?\n",
    "# Logarithmic loss (categorical crossentropy)\n",
    "\n",
    "# How many parameters (i.e. weights and biases, NOT hyper-parameters) does the model have?\n",
    "# The total parameters are 108,618. The result can be obtained by model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 11, 11, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               102528    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 108,618\n",
      "Trainable params: 108,618\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 2.2090\n",
      "Epoch 2/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 1.1778\n",
      "Epoch 3/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.5237\n",
      "Epoch 4/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3213\n",
      "Epoch 5/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2384\n",
      "Epoch 6/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1926\n",
      "Epoch 7/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1527\n",
      "Epoch 8/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1244\n",
      "Epoch 9/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0908\n",
      "Epoch 10/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0785\n",
      "Epoch 11/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0548\n",
      "Epoch 12/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0516\n",
      "Epoch 13/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0366\n",
      "Epoch 14/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0256\n",
      "Epoch 15/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0146\n",
      "Epoch 16/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0106\n",
      "Epoch 17/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0066\n",
      "Epoch 18/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0059\n",
      "Epoch 19/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0045\n",
      "Epoch 20/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0040\n",
      "Epoch 21/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0034\n",
      "Epoch 22/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0031\n",
      "Epoch 23/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0027\n",
      "Epoch 24/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0026\n",
      "Epoch 25/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0023\n",
      "Epoch 26/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0022\n",
      "Epoch 27/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0019\n",
      "Epoch 28/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0018\n",
      "Epoch 29/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0017\n",
      "Epoch 30/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0016\n",
      "Epoch 31/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0015\n",
      "Epoch 32/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0014\n",
      "Epoch 33/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0014\n",
      "Epoch 34/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0013\n",
      "Epoch 35/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0012\n",
      "Epoch 36/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0012\n",
      "Epoch 37/60\n",
      "800/800 [==============================] - 1s 752us/step - loss: 0.0011\n",
      "Epoch 38/60\n",
      "800/800 [==============================] - 1s 868us/step - loss: 0.0011\n",
      "Epoch 39/60\n",
      "800/800 [==============================] - 1s 908us/step - loss: 0.0010\n",
      "Epoch 40/60\n",
      "800/800 [==============================] - 0s 598us/step - loss: 9.9240e-04\n",
      "Epoch 41/60\n",
      "800/800 [==============================] - 1s 726us/step - loss: 9.4092e-04\n",
      "Epoch 42/60\n",
      "800/800 [==============================] - 0s 529us/step - loss: 8.9719e-04\n",
      "Epoch 43/60\n",
      "800/800 [==============================] - 0s 551us/step - loss: 8.6793e-04\n",
      "Epoch 44/60\n",
      "800/800 [==============================] - 1s 767us/step - loss: 8.4747e-04\n",
      "Epoch 45/60\n",
      "800/800 [==============================] - 1s 797us/step - loss: 8.1116e-04\n",
      "Epoch 46/60\n",
      "800/800 [==============================] - 1s 999us/step - loss: 7.8825e-04\n",
      "Epoch 47/60\n",
      "800/800 [==============================] - 0s 579us/step - loss: 7.6733e-04\n",
      "Epoch 48/60\n",
      "800/800 [==============================] - 1s 750us/step - loss: 7.3198e-04\n",
      "Epoch 49/60\n",
      "800/800 [==============================] - 1s 631us/step - loss: 7.1281e-04\n",
      "Epoch 50/60\n",
      "800/800 [==============================] - 0s 578us/step - loss: 6.9412e-04\n",
      "Epoch 51/60\n",
      "800/800 [==============================] - 1s 895us/step - loss: 6.6727e-04\n",
      "Epoch 52/60\n",
      "800/800 [==============================] - 1s 997us/step - loss: 6.5386e-04\n",
      "Epoch 53/60\n",
      "800/800 [==============================] - 1s 758us/step - loss: 6.3994e-04\n",
      "Epoch 54/60\n",
      "800/800 [==============================] - 0s 609us/step - loss: 6.1971e-04\n",
      "Epoch 55/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 6.0025e-04\n",
      "Epoch 56/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 5.8822e-04\n",
      "Epoch 57/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 5.7606e-04\n",
      "Epoch 58/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 5.5900e-04\n",
      "Epoch 59/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 5.3876e-04\n",
      "Epoch 60/60\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 5.3018e-04\n",
      "200/200 [==============================] - 0s 1ms/step\n",
      "Accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow\n",
    "import timeit\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "# Max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "# Max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# To see how many parameters are in the model\n",
    "model.summary()\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_oh_train, batch_size=32, epochs=60)\n",
    "\n",
    "# Evaluate performance\n",
    "test_loss = model.evaluate(X_test, y_oh_test, batch_size=32)\n",
    "\n",
    "predictions = model.predict(X_test, batch_size=32)\n",
    "predictions = np.argmax(predictions, axis=1) # change encoding again\n",
    "print('Accuracy:', (predictions == y_test).sum() / predictions.shape[0]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - train a model\n",
    "\n",
    "A model's performance depends on many factors apart from the model architecture (e.g. type and number of layers) and the dataset. Here you will get to explore some of the factors that affect model performance. Much of the skill in training deep learning models lies in quickly finding good values/options for these choises.\n",
    "\n",
    "In order to observe the learning process it is best to compare the training set loss with the loss on the test set. How to visualize these variables in Keras is described under [Training history visualization](https://keras.io/visualization/#training-history-visualization) in the documentation.\n",
    "\n",
    "You will explore the effect of 1) optimizer, 2) training duration, and 3) dropout (see the question above).\n",
    "\n",
    "When training, an **epoch** is one pass through the full training set.\n",
    "\n",
    "### Question 3\n",
    "\n",
    "* **Vizualize the training**. Use the model above to observe the training process. Train it for 150 epochs and then plot both \"loss\" and \"val_loss\" (i.e. loss on the valiadtion set, here the terms \"validation set\" and \"test set\" are used interchangably, but this is not always true). What is the optimal number of epochs for minimizing the test set loss? \n",
    "    * Remember to first reset the weights (```model.reset_states()```), otherwise the training just continues from where it was stopped earlier.\n",
    "\n",
    "* **Optimizer**. Select three different optimizers and for each find the close-to-optimal hyper-parameter(s). In your answer, include a) your three choises, b) best hyper-parameters for each of the three optimizers and, c) the code that produced the results.\n",
    "    * *NOTE* that how long the training takes varies with optimizer. I.e., make sure that the model is trained for long enough to reach optimal performance.\n",
    "\n",
    "* **Dropout**. Use the best optimizer and do hyper-parameter seach and find the best value for ```Dropout()```.\n",
    "\n",
    "* **Best model**. Combine the what you learned from the above three questions to build the best model. How much better is it than the worst and average models?\n",
    "\n",
    "* **Results on the test set**. When doing this search for good model configuration/hyper-parameter values, the data set was split into *two* parts: a training set and a test set (the term \"validation\" was used interchangably wiht \"test\"). For your final model, is the performance (i.e. accuracy) on the test set representative for the performance one would expect on a previously unseen data set (drawn from the same distribution)? Why?\n",
    "\n",
    "\n",
    "**Good luck!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The answers of questions 3 are as follows:\n",
    "\n",
    "# Vizualize the training: Visualization.ipynb\n",
    "# The optimal number of epochs for minimizing the test set loss is 8 or 9.\n",
    "\n",
    "# Optimizer\n",
    "# a) Adam, Adagrad, Adadelta\n",
    "# b) best learning rate for these optimizers are respectively \n",
    "# c) The codes that search for the optimal learning rates and the results\n",
    "# are in the files below:\n",
    "# Adam: Adam_clr.ipynb Adam_Optimal.ipynb\n",
    "# Adagrad: Adagrad_clr.ipynb Adagrad_Optimal.ipynb\n",
    "# Adadelta: Adadelta_clr.ipynb Adadelta_Optimal.ipynb\n",
    "# as well as a module file imported for cyclical learning rate search: \n",
    "# clr_callback.py\n",
    "# Please put this file in the local working directory when you run the files above.\n",
    "\n",
    "# The method I used to search for optimal learning rate for different optimizers\n",
    "# is called Cyclical Learning Rates Policy (CLR) and LR Range test.\n",
    "# This website introduces this method: \n",
    "# https://www.jeremyjordan.me/nn-learning-rate/\n",
    "\n",
    "# The paper 'Cyclical Learning Rates for Training Neural Networks', Leslie Smith \n",
    "# first proposes a cyclical learning rate schedule \n",
    "# https://arxiv.org/abs/1506.01186 \n",
    "\n",
    "# Cyclical Learning Rates (CLR) has the following advantages:\n",
    "# CLR allows the model to be trained on higher learning rates and converge faster;\n",
    "# During the middle of learning when learning rate is higher, the learning rate\n",
    "# works as regularisation method and keep network from overfitting.\n",
    "# This helps the network to avoid steep areas of loss and land better flatter minima.\n",
    "\n",
    "# The intuition behind this method is that\n",
    "# The optimal learning rate can yield significant decreases in the loss function.\n",
    "# A systematic approach in finding such learning rate is by observing the magnititudes\n",
    "# of loss change with different learning rates.\n",
    "# Using CLR for the learning rates schedule, I plot the graph of the change in \n",
    "# loss function as the learning rates change.\n",
    "# The d/loss minimum in the graph is the point when \n",
    "# the optimal learning rate can yield significant decreases in the loss function.\n",
    "\n",
    "# Dropout: Dropout_opt.ipynb\n",
    "# Best model: Best_Model.ipynb\n",
    "# I choose Adam optimizer to build a best model because it can achieve high accuracy\n",
    "# with shorter time (in seconds) than Adadelta and Adagrad optimizer.\n",
    "\n",
    "# Results on the test set: Best_Model.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
